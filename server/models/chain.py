import joblib
import json
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    precision_score,
    recall_score,
    f1_score,
    multilabel_confusion_matrix,
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import StackingClassifier
import uuid
from lightgbm import LGBMClassifier


def chain_models(file, model_id):
    # Load models from file
    try:
        models = joblib.load("models_with_ids.pkl")
    except FileNotFoundError:
        raise FileNotFoundError("Models file 'models_with_ids.pkl' not found.")

    # Check if models dictionary is empty
    if not models:
        raise ValueError("No models found in 'models_with_ids.pkl'.")

    # Check if the specified model_id exists
    if model_id not in models:
        raise ValueError(f"Model ID '{model_id}' not found.")

    # Get the model and predict
    model_info = models[model_id]
    model = model_info.get("model", None)
    label_encoder = model_info.get("label_encoder", None)
    feature_columns = model_info.get("feature_columns", [])
    target_column = model_info.get("target_column", "")
    separator = model_info.get("separator", ",")
    label = model_info.get("model_info", "Existing")
    type = model_info.get("type", "Hybrid")

    # Load the new data
    data = pd.read_csv(file, sep=separator)

    # Handle missing values
    data = data.ffill()

    if data.empty:
        raise ValueError("No valid data available after handling missing values.")

    # Split the data into features and target
    X = data[feature_columns].values
    y = data[target_column].values

    # Calculate and display malware type distribution
    malware_distribution = data[target_column].value_counts()
    print(f"\n### Malware Type Distribution ###\n{malware_distribution}")

    # Check if the target variable contains categorical string labels
    label_encoder = None
    if y.dtype == "object":
        label_encoder = LabelEncoder()
        y = label_encoder.fit_transform(y)

    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.1, random_state=0
    )

    if label_encoder:
        y_test_decoded = label_encoder.inverse_transform(y_test)
    else:
        y_test_decoded = y_test

    model_metrics = None

    ### Existing Model ###
    if model:
        try:
            model.fit(X_train, y_train)

            model_test_preds = model.predict(X_test)

            model_metrics = calculate_metrics(
                y_test_decoded, model_test_preds, label_encoder
            )

            print("\n### Existing Model Validation Metrics ###")
            print(f"Test Accuracy: {model_metrics['accuracy']}")
            print(f"Precision: {model_metrics['precision']}")
            print(f"Recall: {model_metrics['recall']}")
            print(f"F1 Score: {model_metrics['f1']}")
            print(model_metrics["classification_report"])

        except Exception as e:
            raise ValueError(f"Error training existing model: {str(e)}")

    # Load existing models in the file if exists
    try:
        existing_models = joblib.load("models_with_ids.pkl")
    except FileNotFoundError:
        existing_models = {}

    # Define new models with unique IDs
    model_ids = {}
    if model_metrics:
        model_ids[type] = str(uuid.uuid4())
        existing_models[model_ids[type]] = {
            "model": model,
            "type": type,
            "label": label,
            "label_encoder": label_encoder,
            "feature_columns": feature_columns,
            "target_column": target_column,
            "separator": separator,
        }

    # Save the models with unique IDs back to the file
    joblib.dump(existing_models, "models_with_ids.pkl")

    result = [
        malware_distribution,
        model_metrics if type == "knn" else None,
        model_metrics if type == "lgbm" else None,
        model_metrics if type == "hybrid" else None,
        model_ids,
    ]

    return tuple(result)


def calculate_metrics(y_true, y_pred, label_encoder):
    if label_encoder:
        y_pred_decoded = label_encoder.inverse_transform(y_pred)
    else:
        y_pred_decoded = y_pred

    # Generate accuracies report
    report = get_classification_report(y_true, y_pred_decoded)

    metrics = {
        "accuracy": accuracy_score(y_true, y_pred_decoded),
        "precision": precision_score(
            y_true, y_pred_decoded, average="macro", zero_division=1
        ),
        "recall": recall_score(
            y_true, y_pred_decoded, average="macro", zero_division=1
        ),
        "f1": f1_score(y_true, y_pred_decoded, average="macro", zero_division=1),
        "classification_report": json.dumps(report, separators=(",", ":")),
    }
    return metrics


def get_classification_report(y_true, y_pred_decoded):
    # Generate classification report
    report = classification_report(
        y_true, y_pred_decoded, zero_division=1, output_dict=True
    )

    # Extract class labels from the report
    labels = [
        class_name
        for class_name in report.keys()
        if class_name not in ["accuracy", "macro avg", "weighted avg"]
    ]

    # Generate multilabel confusion matrix with the extracted labels
    mcm = multilabel_confusion_matrix(y_true, y_pred_decoded, labels=labels)

    # Mapped result
    mapped_result = {}

    # Loop through each class in the classification report
    for i, class_name in enumerate(labels):
        metrics_dict = report[class_name]
        tn, fp, fn, tp = mcm[i].ravel()
        total_instances = tp + fn  # Total actual instances of the class
        class_accuracy = tp / total_instances if total_instances > 0 else 0

        # Add accuracy to the metrics_dict for each class
        metrics_dict["accuracy"] = class_accuracy
        mapped_result[class_name] = metrics_dict

    # Add overall accuracy and averages
    mapped_result["overall"] = {
        "accuracy": report["accuracy"],
        "macro_avg": report["macro avg"],
        "weighted_avg": report["weighted avg"],
    }

    return mapped_result

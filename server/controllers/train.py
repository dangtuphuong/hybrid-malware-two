import joblib
import json
import pandas as pd
from datetime import datetime
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    precision_score,
    recall_score,
    f1_score,
    multilabel_confusion_matrix,
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import StackingClassifier
import uuid
from lightgbm import LGBMClassifier


def train_models(
    file,
    separator,
    feature_columns,
    target_column,
    train_models_str="knn,lgbm,hybrid",
):
    sep = separator if separator else ","
    option_models = ""

    if isinstance(train_models_str, str):
        option_models = train_models_str.split(",")

    # Get the current timestamp
    current_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    has_knn = "knn" in option_models
    has_lgbm = "lgbm" in option_models
    has_hybrid = "hybrid" in option_models

    try:
        # Load dataset
        data = pd.read_csv(file, sep=sep)
    except FileNotFoundError:
        raise FileNotFoundError("Dataset file not found.")
    except pd.errors.EmptyDataError:
        raise ValueError("Dataset file is empty.")
    except Exception as e:
        raise ValueError(f"An error occurred while loading the dataset: {str(e)}")

    # Handle missing values
    data = data.ffill()

    if data.empty:
        raise ValueError("No valid data available after handling missing values.")

    # Split the data into features and target
    X = data[feature_columns].values
    y = data[target_column].values

    # Calculate and display malware type distribution
    malware_distribution = data[target_column].value_counts()
    print(f"\n### Malware Type Distribution ###\n{malware_distribution}")

    # Check if the target variable contains categorical string labels
    label_encoder = None
    if y.dtype == "object":
        label_encoder = LabelEncoder()
        y = label_encoder.fit_transform(y)

    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.1, random_state=0
    )

    if label_encoder:
        y_test_decoded = label_encoder.inverse_transform(y_test)
    else:
        y_test_decoded = y_test

    knn_metrics = None
    lgbm_metrics = None
    hybrid_metrics = None

    ### KNN Model ###
    if has_knn or has_hybrid:
        try:
            knn_model = KNeighborsClassifier(
                n_neighbors=5, weights="uniform", algorithm="auto"
            )
            knn_model.fit(X_train, y_train)

            knn_test_preds = knn_model.predict(X_test)

            knn_metrics = calculate_metrics(
                y_test_decoded, knn_test_preds, label_encoder
            )

            print("\n### KNN Validation Metrics ###")
            print(f"Test Accuracy: {knn_metrics['accuracy']}")
            print(f"Precision: {knn_metrics['precision']}")
            print(f"Recall: {knn_metrics['recall']}")
            print(f"F1 Score: {knn_metrics['f1']}")
            print(knn_metrics["classification_report"])

        except Exception as e:
            raise ValueError(f"Error training KNN model: {str(e)}")

    ### LGBM Model ###
    if has_lgbm or has_hybrid:
        try:
            lgbm_model = LGBMClassifier(
                n_estimators=200,
                random_state=0,
                verbose=-1,
            )
            lgbm_model.fit(X_train, y_train)

            lgbm_test_preds = lgbm_model.predict(X_test)

            lgbm_metrics = calculate_metrics(
                y_test_decoded, lgbm_test_preds, label_encoder
            )

            print("\n### LGBM Validation Metrics ###")
            print(f"Test Accuracy: {lgbm_metrics['accuracy']}")
            print(f"Precision: {lgbm_metrics['precision']}")
            print(f"Recall: {lgbm_metrics['recall']}")
            print(f"F1 Score: {lgbm_metrics['f1']}")
            print(lgbm_metrics["classification_report"])

        except Exception as e:
            raise ValueError(f"Error training LGBM model: {str(e)}")

    ### Hybrid Model ###
    if has_hybrid:
        try:
            if knn_metrics and lgbm_metrics:
                hybrid_model = StackingClassifier(
                    estimators=[
                        ("knn", knn_model),
                        ("lgbm", lgbm_model),
                    ],
                    final_estimator=LGBMClassifier(),
                )
                hybrid_model.fit(X_train, y_train)

                hybrid_test_preds = hybrid_model.predict(X_test)

                hybrid_metrics = calculate_metrics(
                    y_test_decoded, hybrid_test_preds, label_encoder
                )

                print("\n### Hybrid Model (Feature Fusion with LGBM) ###")
                print(f"Test Accuracy: {hybrid_metrics['accuracy']}")
                print(f"Precision: {hybrid_metrics['precision']}")
                print(f"Recall: {hybrid_metrics['recall']}")
                print(f"F1 Score: {hybrid_metrics['f1']}")
                print(hybrid_metrics["classification_report"])

        except Exception as e:
            raise ValueError(f"Error training Hybrid model: {str(e)}")

    # Load existing models, if the file exists
    try:
        existing_models = joblib.load("models_with_ids.pkl")
    except FileNotFoundError:
        existing_models = {}

    # Define new models with unique IDs
    model_ids = {}
    if knn_metrics and has_knn:
        model_ids["knn"] = str(uuid.uuid4())
        existing_models[model_ids["knn"]] = {
            "model": knn_model,
            "type": "knn",
            "label": "K-Nearest Neighbors",
            "label_encoder": label_encoder,
            "feature_columns": feature_columns,
            "target_column": target_column,
            "separator": sep,
            "created_at": current_timestamp,
        }

    if lgbm_metrics and has_lgbm:
        model_ids["lgbm"] = str(uuid.uuid4())
        existing_models[model_ids["lgbm"]] = {
            "model": lgbm_model,
            "type": "lgbm",
            "label": "LightGBM",
            "label_encoder": label_encoder,
            "feature_columns": feature_columns,
            "target_column": target_column,
            "separator": sep,
            "created_at": current_timestamp,
        }

    if hybrid_metrics and has_hybrid:
        model_ids["hybrid"] = str(uuid.uuid4())
        existing_models[model_ids["hybrid"]] = {
            "model": hybrid_model,
            "type": "hybrid",
            "label": "Hybrid (Stacking Classifier)",
            "label_encoder": label_encoder,
            "feature_columns": feature_columns,
            "target_column": target_column,
            "separator": sep,
            "created_at": current_timestamp,
        }

    # Save the models with unique IDs back to the file
    joblib.dump(existing_models, "models_with_ids.pkl")

    result = [
        malware_distribution,
        knn_metrics if has_knn else None,
        lgbm_metrics if has_lgbm else None,
        hybrid_metrics if has_hybrid else None,
        model_ids,
    ]

    return tuple(result)


def calculate_metrics(y_true, y_pred, label_encoder):
    if label_encoder:
        y_pred_decoded = label_encoder.inverse_transform(y_pred)
    else:
        y_pred_decoded = y_pred

    # Generate accuracies report
    report = get_classification_report(y_true, y_pred_decoded)

    metrics = {
        "accuracy": accuracy_score(y_true, y_pred_decoded),
        "precision": precision_score(
            y_true, y_pred_decoded, average="macro", zero_division=1
        ),
        "recall": recall_score(
            y_true, y_pred_decoded, average="macro", zero_division=1
        ),
        "f1": f1_score(y_true, y_pred_decoded, average="macro", zero_division=1),
        "classification_report": json.dumps(report, separators=(",", ":")),
    }
    return metrics


def get_classification_report(y_true, y_pred_decoded):
    # Generate classification report
    report = classification_report(
        y_true, y_pred_decoded, zero_division=1, output_dict=True
    )

    # Extract class labels from the report
    labels = [
        class_name
        for class_name in report.keys()
        if class_name not in ["accuracy", "macro avg", "weighted avg"]
    ]

    # Generate multilabel confusion matrix with the extracted labels
    mcm = multilabel_confusion_matrix(y_true, y_pred_decoded, labels=labels)

    # Mapped result
    mapped_result = {}

    # Loop through each class in the classification report
    for i, class_name in enumerate(labels):
        metrics_dict = report[class_name]
        tn, fp, fn, tp = mcm[i].ravel()
        total_instances = tp + fn  # Total actual instances of the class
        class_accuracy = tp / total_instances if total_instances > 0 else 0

        # Add accuracy to the metrics_dict for each class
        metrics_dict["accuracy"] = class_accuracy
        mapped_result[class_name] = metrics_dict

    # Add overall accuracy and averages
    mapped_result["overall"] = {
        "accuracy": report["accuracy"],
        "macro_avg": report["macro avg"],
        "weighted_avg": report["weighted avg"],
    }

    return mapped_result
